{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7Dt5FxUsXVE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b30t0Ee6_of"
      },
      "source": [
        "Dane pochodzą z https://dane.gov.pl/pl/dataset/188,wykaz-urzedowych-nazw-miejscowosci-i-ich-czesci"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vij_d2rs04W"
      },
      "outputs": [],
      "source": [
        "#ściągnięcie danych z GitHub\n",
        "url='https://raw.githubusercontent.com/NXTRSS/RNNCourse/main/Wykaz_urzędowych_nazw_miejscowości_stan_na_1.1.2019.csv'\n",
        "s=requests.get(url).content\n",
        "df_cities=pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
        "df_cities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOHYmfAv4Nvg"
      },
      "outputs": [],
      "source": [
        "df_cities['Rodzaj'].value_counts()[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AouDXIyS7c5m"
      },
      "source": [
        "Każdą miejscowośc poprzedzamy tokenem sos i kończymy eos. W naszym przypadku umawiamy sie na $ i ^ - ponieważ model będzie oparty o znaki i wygodniej jet kiedy tokeny specjalne również są znakami."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yx001Sk3tT53"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "<sos>Warszawa</eos>\n",
        "$Warszawa^\n",
        "\n",
        "$Warszawa\n",
        "Warszawa^\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6qkSyiy8THy"
      },
      "source": [
        "Do modelu wybieramy tylko nazwy miast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CL0tJv7jtAKb"
      },
      "outputs": [],
      "source": [
        "list_of_cities = df_cities[df_cities['Rodzaj'] == 'miasto']['Nazwa miejscowości'].tolist()\n",
        "#PROSZE UZUPEŁNIĆ: dodać znak $ przed każdym miastem i ^ na końcu (czyli chcemy aby Warszawa -> $Warszawa^)\n",
        "\n",
        "\n",
        "list_of_cities[:10]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73WWGWCB8V04"
      },
      "source": [
        "Dane będą dopełnione zerami do najdłuższej nazwy w datasecie. Powoduje to pewnien problem - krótsze nazwy będą składały się w dużej mierze z zer (paddingu). W takiej sytuacju model będzie uczył się że pad trzeba zamienić na pad. Nie jest to zła wiedza natomiast rodzi problem - zamiana pad na pad jest bardzo łatwa i nie jest nam do niczego potrzebna. Jeżeli tokeny pad będą uwzględniane przy liczneiu loss to będzie ona sztucznie zaniżona. Np. dla nazwy która w połowie składa się z paddingu, loss będzie zaniżony o połowę. Dlatego w późniejszej części ustawiamy maskowanie - tj ignorowanie klasy o indeksie 0. Nie będzie ona uwzględniana przy liczeniu wartości funkcji straty co da nam bardziej realistyczny obraz sytuacji."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXQT6RJ3xTnN"
      },
      "source": [
        "```\n",
        "$Baborów^0000000000000\n",
        "$Baranów Sandomierski^\n",
        "\n",
        "$Baranów Sandomierski\n",
        "Baranów Sandomierski^\n",
        "\n",
        "$Baborów000000000000\n",
        "Baborów^000000000000\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meLTQJrGuYdz"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import TextVectorization, Embedding, LSTM, TimeDistributed, Dense, Dropout, GRU\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "from keras.activations import softmax\n",
        "import tensorflow.keras as keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfJNZ85Pun-x"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(filters='', lower=False, char_level=True) # resetuejmy filtr bo domyślnie usunie tokeny $ i ^, nie zmniejszamy liter - model ma się nauczyć poprawnej wielkości\n",
        "#PROSZĘ UZUPEŁNIĆ: proszę wytrenować tokenizer na naszej liście miast\n",
        "\n",
        "\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)\n",
        "# tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_q68k3Y4Nvk"
      },
      "outputs": [],
      "source": [
        "tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe15GpVqvVHk"
      },
      "outputs": [],
      "source": [
        "#PROSZĘ UZUPEŁNIĆ: przygotować dane X_train poprzez podane nazwy miasta bez ostatniego znaku np. $Warszawa^ -> $Warszawa\n",
        "X_train = None\n",
        "#przygotować dane X_train poprzez podane nazwy miasta bez pierwszego znaku np. $Warszawa^ -> Warszawa^\n",
        "y_train = None\n",
        "\n",
        "print(f'example input: {X_train[0]}, example output: {y_train[0]}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#proszę użyć tokenizera na X_train i y_train\n",
        "X_train = None\n",
        "y_train = None\n",
        "\n",
        "#proszę dodać padding (jego długość ustali się automatycznie po podaniu danych), tak aby 0 były dodane na końcu sekwencji\n",
        "X_train = None\n",
        "y_train = None\n",
        "\n",
        "print(f'example input: {X_train[0]}, example output: {y_train[0]}')"
      ],
      "metadata": {
        "id": "9gIEBQEu6uVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hh5UT8R4Nvk"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rn97HFflvyYZ"
      },
      "outputs": [],
      "source": [
        "class LanguageModel(keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__(self)\n",
        "        #proszę zadeklarować warstę embedding z odpowiednim kszatłtem, \n",
        "        #wielkością wektora 32 oraz ustawieniem parametru uwzględniającego maskę zer\n",
        "        self.embedding = None\n",
        "\n",
        "        #Proszę zadeklarować warstę GRU z 48 jednostkami oraz ze zwracaniem \n",
        "        #sekwencji i stanu ukrytego\n",
        "        self.rnn = None\n",
        "\n",
        "        #proszę zadeklarować warstę gęstą z odpowiednią liczbą wyjścia (jaka \n",
        "        #jest liczba klas?) oraz funkcją aktywacji softmax\n",
        "        self.fc = keras.layers.Dense(vocab_size, activation='softmax')\n",
        "    \n",
        "    def call(self, inputs, training=False, hidden_state=None, return_state=False): # pamiętajcie, że domyślne wartości argumentów w call powinny być ustawione do infernecji\n",
        "        \"\"\"\n",
        "        inputs - wejście w formacie [batch size x max seq len]\n",
        "        training - czy w trakcie teningu\n",
        "        hidden_state - initial state od którego sieć ma zacząć. Jeżeli None - domyślny start\n",
        "        retur_state - czy zwrócić ostatni hidden state?\n",
        "        \"\"\"\n",
        "        x = inputs\n",
        "        x = self.embedding(x, training=training) # embedujemy\n",
        "\n",
        "        if hidden_state is None: \n",
        "            hidden_state = self.rnn.get_initial_state(inputs=x) # pobieramy domyślny hidden state jeżeli nie podano żadnego. Zwykle jest to tensor zer\n",
        "\n",
        "        seq_out, last_hidden_state = self.rnn(x, training=training, initial_state=hidden_state)\n",
        "        x = self.fc(seq_out) # mamy predykcję znaku (a raczej jej rozkład)\n",
        "\n",
        "        if return_state: # zwracamy predykcję i ostatni state\n",
        "            return x, last_hidden_state\n",
        "        return x #zwracamy tylko predykcję\n",
        "\n",
        "model = LanguageModel()\n",
        "model(X_train)\n",
        "# model.build(input_shape=[None, 1])\n",
        "model.summary()\n",
        "criterion = keras.losses.SparseCategoricalCrossentropy()\n",
        "model.compile(optimizer='adam', loss=criterion)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ph_I_RW94Nvl"
      },
      "outputs": [],
      "source": [
        "# proszę wytrenować model na odpowiednich danych przez 100 epok\n",
        "history = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7en-oRvFELDn"
      },
      "outputs": [],
      "source": [
        "# Próbkowanie litera po literze (znak po znaku)\n",
        "\n",
        "prompt = \"$A\" # ustalamy prompt\n",
        "encoded_prompt = tokenizer.texts_to_sequences([prompt]) # enkodujemy go\n",
        "encoded_prompt = tf.convert_to_tensor(encoded_prompt) # i zamieniamy na tensor\n",
        "hidden_state = None # pierwszy hidden state zostawiamy domyślny\n",
        "\n",
        "for i in range(30): # nie więcej niż 30 znaków\n",
        "    model_out, hidden_state = model(encoded_prompt, return_state=True, hidden_state=hidden_state) # pobieramy predykcje modelu oraz ostatni hidden state. Zwróć uwagę\n",
        "    # że przekazujemy hidden state\n",
        "    last_output = model_out[0, -1, :].numpy() # wybieramy predykcję ostatniego znaku\n",
        "    next_char_idx = np.random.choice(np.arange(len(last_output)),  p=last_output) # i samplujemy zgodnie z rozkładem prawdopodobieństwa\n",
        "    next_char = tokenizer.index_word[next_char_idx] # zamieniamy indeks znaku na znak\n",
        "    prompt += next_char # i dopisujemy do prompt\n",
        "\n",
        "    encoded_prompt = tokenizer.texts_to_sequences([next_char]) # nowe wejście do modelu składa się TYLKO z jednego, nowowygenerowanego znaku\n",
        "    # ponieważ informacja o dotychczasowje sekwencji zawarta jest w hidden state którym zarządamy manualnie w przeciwieństgwie do uproszczonej metody\n",
        "#     encoded_prompt = tokenizer.texts_to_sequences([prompt])\n",
        "    encoded_prompt = tf.convert_to_tensor(encoded_prompt)\n",
        "\n",
        "    if next_char == '^':\n",
        "        break\n",
        "\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2LhKv7G6GXV"
      },
      "outputs": [],
      "source": [
        "# próbkowanie/samplowanie modelu\n",
        "# ustalamy wartość początkową (nazywaną seed/prompt/warmpup text)\n",
        "\n",
        "prompt = \"$\" # tylko <sos> - niech modlel wypluje nazwe na dowolną literę\n",
        "\n",
        "for i in range(30): # jeżeli przez 30 znaków model nie uzna za stosowne wyprodukować <EOS> oznaczający koniec, przerywamy. \n",
        "    encoded_prompt = tokenizer.texts_to_sequences([prompt]) # enkodujemy prompt\n",
        "    output = model.predict(encoded_prompt) # generujemy predykcję znaków\n",
        "    last_output = output[0, -1, :] # interesuje nas tylko ostatni. Zwróć uwage że operujemy na batch_size=1\n",
        "    # next_char_idx = last_output.argmax() - greedy sampling - wybierz zawsze najbardziej prawdopodbny znak. W takim scenariuszu generowane nazwy będą zawsze takie same!\n",
        "    next_char_idx = np.random.choice(np.arange(len(last_output)),  p=last_output) # propabilistic samplig - wybierz znak zgodnie z rozkładem prawdopodobieństwa zwróconym przez model\n",
        "    # jest jeszcze beam search - najbardziej zaawansowana metoda której nie omawialiśmy\n",
        "    next_char = tokenizer.index_word[next_char_idx] # zamień indeks znaku na konkretny znak\n",
        "    prompt += next_char # i dopisz go do prompt, \n",
        "\n",
        "    if next_char == '^': # jeżeli model uznał że skończył to kończymy samplowanie\n",
        "        break\n",
        "\n",
        "print(prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwVRJeci8zWj"
      },
      "outputs": [],
      "source": [
        "np.random.choice(np.arange(len(last_output)),  p=last_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoPyCOpI7ZXy"
      },
      "outputs": [],
      "source": [
        "for c, i in tokenizer.word_index.items():\n",
        "    prob = last_output[i]\n",
        "    if prob > 0.001:\n",
        "        print(f'{c}:{prob:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO0_U70w3EJP"
      },
      "source": [
        "```\n",
        "X = Warszaw (a)\n",
        "\n",
        "y = model(X)\n",
        "\n",
        "a - 0.75\n",
        "  - 0.1\n",
        "- - 0.1\n",
        "\n",
        "X2 = Warszawa\n",
        "y2 = model(X2) -> ^\n",
        "\n",
        "X = $ y = W   O(1)\n",
        "$W, a         O(2)\n",
        "$Wa, r        O(3)\n",
        "\n",
        "...\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yIMbRBK2clQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pShGBliSzsxf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Model_języka_nazwy_miast-plain.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "mlp",
      "language": "python",
      "name": "mlp"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}