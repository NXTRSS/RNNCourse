{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqxkVD5maZd5"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "keras.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-vZfTTaaZeA"
      },
      "source": [
        "# Rekurencyjne sieci neuronowe\n",
        "\n",
        "\n",
        "## Warstwa rekurencyjna w pakiecie Keras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_aCUilHaZeB"
      },
      "outputs": [],
      "source": [
        "from keras import Sequential\n",
        "from keras.layers import SimpleRNN, Dense, Embedding, GRU, Bidirectional\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krg9AmNVaZeC"
      },
      "source": [
        "Powyższa warstwa przyjmuje obiekty wejściowe o kształcie (rozmiar_wsadu, kroki_czasu, cechy_wejściowe).\n",
        "\n",
        "Warstwa SimpleRNN, podobnie jak wszystkie rekurencyjne warstwy pakietu Keras, może być uruchomiona w dwóch trybach: może zwracać pełne sekwencje kolejnych obiektów wyjściowych dla każdego kroku czasu (trójwymiarowe tensory o kształcie (rozmiar_wsadu, kroki_czasu, cechy_wyjściowe)) lub tylko ostatnie obiekty wyjściowe poszczególnych sekwencji wejściowych (dwuwymiarowe tensory o kształcie (rozmiar_wsadu, cechy_wyjściowe)). Wybór trybu pracy jest dokonywany za pomocą argumentu return_sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "438ch59iaZeF"
      },
      "source": [
        "Teraz użyjmy modelu RNN w celu rozwiązania problemu klasyfikacji recenzji filmów. Zacznijmy od wstępnej obróbki danych:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zB9XO6f9aZeF"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_features = 10000  # Liczba słów traktowanych jako cechy.\n",
        "maxlen = 500  # Ucina recenzje po tej liczbie słów.\n",
        "batch_size = 128 # wielkość batcha\n",
        "rnn_units = 32 # Liczba neuronów jaką będziemy wykorzystywać\n",
        "max_samples = 25000 # Liczba danych treningowych *można podać mniej niż 25 000 aby sieć uczyła się szybciej\n",
        "\n",
        "print('Ładowanie danych...')\n",
        "(input_train_org, y_train), (input_test_org, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(input_train_org), 'sekwencje treningowe')\n",
        "print(len(input_test_org), 'sekwencje testowe')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNBm22hh877w"
      },
      "outputs": [],
      "source": [
        "#histogram długości sekwencji\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist([len(x) for x in input_train_org])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afTc-N43877x"
      },
      "outputs": [],
      "source": [
        "print('Sekwencje (próbki x czas)')\n",
        "input_train_org = input_train_org[:max_samples]\n",
        "input_test_org = input_test_org[:max_samples]\n",
        "input_train = pad_sequences(input_train_org, maxlen=maxlen)\n",
        "input_test = pad_sequences(input_test_org, maxlen=maxlen)\n",
        "print('Kształt obiektu input_train:', input_train.shape)\n",
        "print('Kształt obiektu input_test:', input_test.shape)\n",
        "y_train = y_train[:max_samples]\n",
        "y_test = y_test[:max_samples]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8G3ledDaZeG"
      },
      "source": [
        "Przeprowadźmy proces trenowania prostej rekurencyjnej sieci przy użyciu warstwy Embedding i warstwy SimpleRNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrTd0k-SaZeG",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "model = Sequential()\n",
        "# proszę dodać następujące warstwy:\n",
        "# - warstwa Embedding z odpowiednią licznością wejścia (dla ilu słów musimy wyznaczyć reprezentację wektorową?)\n",
        "# i rnn_units jako wielkość wektora\n",
        "# - warstwa SimpleRNN z rnn_units jako liczbą neuronów\n",
        "# - gęstą warstwę z odpowiednią liczbą neuronów (patrz wyjście) oraz funkcją aktywacji sigmoid\n",
        "\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPR2VF1_877z"
      },
      "outputs": [],
      "source": [
        "# proszę skompilować model z oprimizerem RMSprop, odpowiednią funkcją kosztu oraz dokładnością jako metrykę do monitorowania\n",
        "\n",
        "# proszę wytrenować model, liczba epok 10, użyć zmiennej batch_size, ustawić validation_split na 0.2, można też spróbować \n",
        "# ustawić liczbę instacji do obliczania (workers)\n",
        "history = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5hB5aLaaZeH"
      },
      "source": [
        "Teraz możemy wyświetlić wykresy dokładności i straty w procesach trenowania i walidacji:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRo7ellbaZeH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Dokladnosc trenowania')\n",
        "plt.plot(epochs, val_acc, 'b', label='Dokladnosc walidacji')\n",
        "plt.title('Dokladnosc trenowania i walidacji')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Strata trenowania')\n",
        "plt.plot(epochs, val_loss, 'b', label='Strata walidacji')\n",
        "plt.title('Strata trenowania i walidacji')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtvMVQi4aZeI"
      },
      "source": [
        "[...]\n",
        "\n",
        "## Przykład warstwy LSTM zaimplementowanej w pakiecie Keras\n",
        "\n",
        "Czas przyjrzeć się praktycznemu przykładowi zastosowania warstwy LSTM. Skonfigurujemy model, w którym znajdzie się taka warstwa, i wytrenujemy go na zbiorze danych IMDB. Przypomina on zaprezentowany wcześniej model z warstwą SimpleRNN. Określimy tylko liczbę wymiarów obiektu wyjściowego warstwy LSTM. Pozostałe argumenty tej warstwy (jest ich wiele) pozostawimy przy wartościach domyślnych. Ustawienia domyślne pakietu Keras są przemyślane i zwykle „po prostu działają” bez konieczności poświęcania dużej ilości czasu na ręczne dostrajanie parametrów."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCenWNtZaZeI"
      },
      "outputs": [],
      "source": [
        "from keras.layers import LSTM\n",
        "\n",
        "model = Sequential()\n",
        "# proszę dodać następujące warstwy:\n",
        "# - warstwa Embedding z odpowiednią licznością wejścia (dla ilu słów musimy wyznaczyć reprezentację wektorową?)\n",
        "# i rnn_units jako wielkość wektora\n",
        "# - warstwa LSTM z rnn_units jako liczbą neuronów\n",
        "# - gęstą warstwę z odpowiednią liczbą neuronów (patrz wyjście) oraz funkcją aktywacji sigmoid\n",
        "\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# proszę skompilować model z oprimizerem RMSprop, odpowiednią funkcją kosztu oraz dokładnością jako metrykę do monitorowania\n",
        "\n",
        "# proszę wytrenować model, liczba epok 10, użyć zmiennej batch_size, ustawić validation_split na 0.2, można też spróbować \n",
        "# ustawić liczbę instacji do obliczania (workers)\n",
        "history = None"
      ],
      "metadata": {
        "id": "R36xLES7SuNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-M7OKm-0aZeI"
      },
      "outputs": [],
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Dokladnosc trenowania')\n",
        "plt.plot(epochs, val_acc, 'b', label='Dokladnosc walidacji')\n",
        "plt.title('Dokladnosc trenowania i walidacji')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Strata trenowania')\n",
        "plt.plot(epochs, val_loss, 'b', label='Strata walidacji')\n",
        "plt.title('Strata trenowania i walidacji')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h5xlrWS9oqz"
      },
      "source": [
        "## Korzystanie z dwukierunkowych rekurencyjnych sieci neuronowych\n",
        "\n",
        "\n",
        "Ostatnim rozwiązaniem, które chciałbym wprowadzić w tym podrozdziale, są dwukierunkowe rekurencyjne sieci neuronowe. Są to popularne wersje sieci rekurencyjnych, które w przypadku niektórych zadań charakteryzują się bardzo dobrą wydajnością. Stosuje się je często w przetwarzaniu języka naturalnego. Można określić je mianem uniwersalnego wytrychu sprawdzającego się podczas pracy nad problemami związanymi z przetwarzaniem języka naturalnego.\n",
        "\n",
        "Rekurencyjne sieci neuronowe przetwarzają zależności wynikające z kolejności lub upływu czasu — przetwarzają one sekwencje w sposób uporządkowany. Zmiana kolejności obserwacji lub jej odwrócenie może całkowicie zmienić reprezentacje tworzone przez tego typu sieci. To właśnie z tego powodu sieci te sprawdzają się dobrze podczas rozwiązywania problemów, w których kolejność ma znaczenie (przykładem takiego problemu jest prognozowanie temperatury). Dwukierunkowe rekurencyjne sieci neuronowe korzystają z tej własności sieci rekurencyjnych. Składają się one z dwóch standardowych sieci rekurencyjnych (np. warstw GRU lub LSTM). Każda z tych sieci składowych przetwarza sekwencję wejściową w innym kierunku (chronologicznym lub przeciwnym do chronologicznego). Utworzone przez nie reprezentacje są następnie łączone. Dwukierunkowa sieć rekurencyjna, przetwarzając sekwencję w obu kierunkach, może wyłapać zależności, które nie są dostrzegane przez jednokierunkową sieć rekurencyjną.\n",
        "\n",
        "Oczywiście chronologiczne przetwarzanie sekwencji przez zaprezentowane dotychczas warstwy sieci rekurencyjnych jest naszym nieświadomym wyborem (na razie nie mieliśmy okazji, aby podważyć jego sensowność). Czy sieci rekurencyjne działałyby wystarczająco dobrze, gdyby przetwarzały sekwencje wejściowe w kierunku odwrotnym do kolejności chronologicznej? Sprawdźmy to w praktyce, Wystarczy utworzyć generator, który odwraca sekwencję wejściową według wymiaru będącego osią czasu (musimy zastąpić ostatnią linię kodu następującym rozwiązaniem: yield samples[:, ::-1, :], targets). Po wytrenowaniu tej samej sieci z jedną warstwą GRU (z warstwy tej korzystaliśmy w pierwszym eksperymencie) uzyskamy następujące wyniki:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKVU9jLS9iQ2"
      },
      "outputs": [],
      "source": [
        "# odwrócenie naszej sekwencji\n",
        "input_train_rev = np.array([x[::-1] for x in input_train_org])\n",
        "input_test_rev = np.array([x[::-1] for x in input_test_org])\n",
        "\n",
        "input_train_rev = pad_sequences(input_train_rev, maxlen=maxlen)\n",
        "input_test_rev = pad_sequences(input_test_rev, maxlen=maxlen)\n",
        "\n",
        "model = Sequential()\n",
        "# proszę dodać następujące warstwy:\n",
        "# - warstwa Embedding z odpowiednią licznością wejścia (dla ilu słów musimy wyznaczyć reprezentację wektorową?)\n",
        "# i rnn_units jako wielkość wektora\n",
        "# - warstwa LSTM z rnn_units jako liczbą neuronów\n",
        "# - gęstą warstwę z odpowiednią liczbą neuronów (patrz wyjście) oraz funkcją aktywacji sigmoid\n",
        "\n",
        "# proszę skompilować model z oprimizerem RMSprop, odpowiednią funkcją kosztu oraz dokładnością jako metrykę do monitorowania\n",
        "\n",
        "# proszę wytrenować model, liczba epok 10, użyć zmiennej batch_size, ustawić validation_split na 0.2, można też spróbować \n",
        "# ustawić liczbę instacji do obliczania (workers)\n",
        "history = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30DtOFhj-wKB"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Dokladnosc trenowania')\n",
        "plt.plot(epochs, val_acc, 'b', label='Dokladnosc walidacji')\n",
        "plt.title('Dokladnosc trenowania i walidacji')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Strata trenowania')\n",
        "plt.plot(epochs, val_loss, 'b', label='Strata walidacji')\n",
        "plt.title('Strata trenowania i walidacji')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km34_KHA-4gX"
      },
      "source": [
        "W celu utworzenia instancji dwukierunkowej sieci rekurencyjnej należy skorzystać z warstwy Bidirectional zaimplementowanej w pakiecie Keras, która jako swój pierwszy argument przyjmuje instancję warstwy rekurencyjnej. Funkcja Bidirectional tworzy drugą oddzielną instancję tej warstwy. Jedna instancja jest używana do przetwarzania sekwencji wejściowej w kierunku chronologicznym, a druga instancja jest używana do przetwarzania odwróconej sekwencji wejściowej. Wypróbujmy działanie tego rozwiązania w celu rozwiązania problemu analizy sentymentu recenzji filmów wchodzących w skład zbioru IMDB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rO1ecliP-5B4"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "# proszę dodać następujące warstwy:\n",
        "# - warstwa Embedding z odpowiednią licznością wejścia (dla ilu słów musimy wyznaczyć reprezentację wektorową?)\n",
        "# i rnn_units jako wielkość wektora\n",
        "# - warstwa Bidirectional LSTM z rnn_units jako liczbą neuronów\n",
        "# - gęstą warstwę z odpowiednią liczbą neuronów (patrz wyjście) oraz funkcją aktywacji sigmoid\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# proszę skompilować model z oprimizerem RMSprop, odpowiednią funkcją kosztu oraz dokładnością jako metrykę do monitorowania\n",
        "\n",
        "# proszę wytrenować model, liczba epok 10, użyć zmiennej batch_size, ustawić validation_split na 0.2, można też spróbować \n",
        "# ustawić liczbę instacji do obliczania (workers)\n",
        "history = None"
      ],
      "metadata": {
        "id": "rgWDODofT4BN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eJuv_XWBNGU"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Dokladnosc trenowania')\n",
        "plt.plot(epochs, val_acc, 'b', label='Dokladnosc walidacji')\n",
        "plt.title('Dokladnosc trenowania i walidacji')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Strata trenowania')\n",
        "plt.plot(epochs, val_loss, 'b', label='Strata walidacji')\n",
        "plt.title('Strata trenowania i walidacji')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRmwa1BtDwbt"
      },
      "source": [
        "# Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xtgaTSTnBNNn"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "# proszę dodać następujące warstwy:\n",
        "# - warstwa Embedding z odpowiednią licznością wejścia (dla ilu słów musimy wyznaczyć reprezentację wektorową?)\n",
        "# i rnn_units jako wielkość wektora\n",
        "# - warstwa GRU z rnn_units jako liczbą neuronów oraz dropoutem 0.2 i recurent dropout 0.2\n",
        "# - gęstą warstwę z odpowiednią liczbą neuronów (patrz wyjście) oraz funkcją aktywacji sigmoid\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# proszę skompilować model z oprimizerem RMSprop, odpowiednią funkcją kosztu oraz dokładnością jako metrykę do monitorowania\n",
        "\n",
        "# proszę wytrenować model, liczba epok 10, użyć zmiennej batch_size, ustawić validation_split na 0.2, można też spróbować \n",
        "# ustawić liczbę instacji do obliczania (workers)\n",
        "history = None"
      ],
      "metadata": {
        "id": "4cI91l2BVKTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OuAAlsPfEjEz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Dokladnosc trenowania')\n",
        "plt.plot(epochs, val_acc, 'b', label='Dokladnosc walidacji')\n",
        "plt.title('Dokladnosc trenowania i walidacji')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Strata trenowania')\n",
        "plt.plot(epochs, val_loss, 'b', label='Strata walidacji')\n",
        "plt.title('Strata trenowania i walidacji')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rq3g3vOPEub6"
      },
      "source": [
        "# Tworzenie stosów warstw rekurencyjnych"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwJMNs8MEve2"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "# proszę dodać następujące warstwy:\n",
        "# - warstwa Embedding z odpowiednią licznością wejścia (dla ilu słów musimy wyznaczyć reprezentację wektorową?)\n",
        "# i rnn_units jako wielkość wektora\n",
        "# - warstwa GRU z rnn_units jako liczbą neuronów, przekazywaniem w głąb sieci sekwencji\n",
        "# oraz dropoutem 0.1 i recurent dropout 0.5\n",
        "# - warstwa GRU z rnn_units jako liczbą neuronów oraz dropoutem 0.2 i recurent dropout 0.4\n",
        "# - gęstą warstwę z odpowiednią liczbą neuronów (patrz wyjście) oraz funkcją aktywacji sigmoid\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# proszę skompilować model z oprimizerem RMSprop, odpowiednią funkcją kosztu oraz dokładnością jako metrykę do monitorowania\n",
        "\n",
        "# proszę wytrenować model, liczba epok 10, użyć zmiennej batch_size, ustawić validation_split na 0.2, można też spróbować \n",
        "# ustawić liczbę instacji do obliczania (workers)\n",
        "history = None"
      ],
      "metadata": {
        "id": "kNGxvvYZU5UM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHKwP8GTFDHX"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Dokladnosc trenowania')\n",
        "plt.plot(epochs, val_acc, 'b', label='Dokladnosc walidacji')\n",
        "plt.title('Dokladnosc trenowania i walidacji')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Strata trenowania')\n",
        "plt.plot(epochs, val_loss, 'b', label='Strata walidacji')\n",
        "plt.title('Strata trenowania i walidacji')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1twaBXaaxL9h"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "sieci_rekurencyjne-plain.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "nlp"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}