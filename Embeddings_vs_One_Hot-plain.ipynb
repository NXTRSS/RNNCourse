{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91cce6f4",
      "metadata": {
        "id": "91cce6f4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras import Input\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# paczki do modelowania\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import regularizers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Funkcje pomocnicze\n",
        "def get_closest(x, embeddings, topn=3):\n",
        "    \"\"\"\n",
        "    Get the closest embeddings calculating the euclidean distance\n",
        "    Parameters\n",
        "    ----------\n",
        "    x: np.ndarray\n",
        "      Vector containing an embedding\n",
        "    top_k: int, optional\n",
        "      Get the top k similar embeddings\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "      Dict containing the top k similar embeddings to the given x\n",
        "    \"\"\"\n",
        "    # Stack all embeddings in a single matrix. Note: the matrix dimention will be\n",
        "    # V x D where V is the vocabulary size and D is the embedding dimension\n",
        "    embedding_matrix = np.array(list(embeddings.values()))\n",
        "    # Using broadcasting compute distance to each embedding in our vocabulary\n",
        "    distances = x - embedding_matrix\n",
        "    # Comoute the magnitude of each distance\n",
        "    distances = np.linalg.norm(distances, axis=1)\n",
        "    # Sort distance and keep the smallest k\n",
        "    min_idx = np.argsort(distances)[:topn]\n",
        "    return [list(embeddings)[i] for i in min_idx]"
      ],
      "metadata": {
        "id": "gzg6-Il4eTQB"
      },
      "id": "gzg6-Il4eTQB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83659d77",
      "metadata": {
        "id": "83659d77"
      },
      "outputs": [],
      "source": [
        "#ściagnięcie danych\n",
        "nltk.download('stopwords')\n",
        "url = \"https://raw.githubusercontent.com/ashutoshmakone/Twitter-US-Airline-Sentiment-classification/main/Dataset/Tweets.csv\"\n",
        "df = pd.read_csv(url, sep=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be13f040",
      "metadata": {
        "id": "be13f040"
      },
      "outputs": [],
      "source": [
        "NB_WORDS = 10000  # Parametr odpowiadający za maksymalną liczbę słów w słowniku (najczęstsze)\n",
        "NB_START_EPOCHS = 10  # Liczba epok podczas treningu\n",
        "BATCH_SIZE = 512  # Wielkość mini-batcha\n",
        "MAX_LEN = 24  # Maksymalna długość sekwencji \n",
        "GLOVE_DIM = 300  # Wymiarowość embeddinga GloVe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa26d333",
      "metadata": {
        "id": "fa26d333"
      },
      "outputs": [],
      "source": [
        "def deep_model(model, X_train, y_train, X_valid, y_valid):\n",
        "    '''\n",
        "    Function to train a multi-class model. The number of epochs and \n",
        "    batch_size are set by the constants at the top of the\n",
        "    notebook. \n",
        "    \n",
        "    Parameters:\n",
        "        model : model with the chosen architecture\n",
        "        X_train : training features\n",
        "        y_train : training target\n",
        "        X_valid : validation features\n",
        "        Y_valid : validation target\n",
        "    Output:\n",
        "        model training history\n",
        "    '''\n",
        "    model.compile(optimizer='rmsprop'\n",
        "                  , loss='categorical_crossentropy'\n",
        "                  , metrics=['accuracy'])\n",
        "    \n",
        "    history = model.fit(X_train\n",
        "                       , y_train\n",
        "                       , epochs=NB_START_EPOCHS\n",
        "                       , batch_size=BATCH_SIZE\n",
        "                       , validation_data=(X_valid, y_valid)\n",
        "                       , verbose=2)\n",
        "    return history\n",
        "\n",
        "\n",
        "def eval_metric(history, metric_name):\n",
        "    '''\n",
        "    Function to evaluate a trained model on a chosen metric. \n",
        "    Training and validation metric are plotted in a\n",
        "    line chart for each epoch.\n",
        "    \n",
        "    Parameters:\n",
        "        history : model training history\n",
        "        metric_name : loss or accuracy\n",
        "    Output:\n",
        "        line chart with epochs of x-axis and metric on\n",
        "        y-axis\n",
        "    '''\n",
        "    metric = history.history[metric_name]\n",
        "    val_metric = history.history['val_' + metric_name]\n",
        "\n",
        "    e = range(1, NB_START_EPOCHS + 1)\n",
        "\n",
        "    plt.plot(e, metric, 'bo', label='Train ' + metric_name)\n",
        "    plt.plot(e, val_metric, 'b', label='Validation ' + metric_name)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def test_model(model, X_train, y_train, X_test, y_test, epoch_stop):\n",
        "    '''\n",
        "    Function to test the model on new data after training it\n",
        "    on the full training data with the optimal number of epochs.\n",
        "    \n",
        "    Parameters:\n",
        "        model : trained model\n",
        "        X_train : training features\n",
        "        y_train : training target\n",
        "        X_test : test features\n",
        "        y_test : test target\n",
        "        epochs : optimal number of epochs\n",
        "    Output:\n",
        "        test accuracy and test loss\n",
        "    '''\n",
        "    model.fit(X_train\n",
        "              , y_train\n",
        "              , epochs=epoch_stop\n",
        "              , batch_size=BATCH_SIZE\n",
        "              , verbose=2)\n",
        "    results = model.evaluate(X_test, y_test)\n",
        "    \n",
        "    return results\n",
        "\n",
        "def remove_stopwords(input_text):\n",
        "    '''\n",
        "    Function to remove English stopwords from a Pandas Series.\n",
        "    \n",
        "    Parameters:\n",
        "        input_text : text to clean\n",
        "    Output:\n",
        "        cleaned Pandas Series \n",
        "    '''\n",
        "    stopwords_list = stopwords.words('english')\n",
        "    # Some words which might indicate a certain sentiment are kept via a whitelist\n",
        "    whitelist = [\"n't\", \"not\", \"no\"]\n",
        "    words = input_text.split() \n",
        "    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
        "    return \" \".join(clean_words) \n",
        "    \n",
        "def remove_mentions(input_text):\n",
        "    '''\n",
        "    Function to remove mentions, preceded by @, in a Pandas Series\n",
        "    \n",
        "    Parameters:\n",
        "        input_text : text to clean\n",
        "    Output:\n",
        "        cleaned Pandas Series \n",
        "    '''\n",
        "    return re.sub(r'@\\w+', '', input_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f940e0ed",
      "metadata": {
        "id": "f940e0ed"
      },
      "outputs": [],
      "source": [
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe95ccb6",
      "metadata": {
        "id": "fe95ccb6"
      },
      "outputs": [],
      "source": [
        "#wybranie z danych tylko kolumny z tekstem i kolumny z sentymentem\n",
        "df = df.reindex(np.random.permutation(df.index))  \n",
        "df = df[['text', 'airline_sentiment']]\n",
        "df.text = df.text.apply(remove_stopwords).apply(remove_mentions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa8d5ea3",
      "metadata": {
        "id": "aa8d5ea3"
      },
      "outputs": [],
      "source": [
        "#PROSZĘ UZUPEŁNIĆ: Podział na próbkę treningową i testową\n",
        "#wielkość probki testowej - 10%, ziarno dla losowania - 37\n",
        "X_train, X_test, y_train, y_test = None\n",
        "print('# Train data samples:', X_train.shape[0])\n",
        "print('# Test data samples:', X_test.shape[0])\n",
        "#ograniczenie liczby próbek ze względu na Colaba (jak puszczane lokalnie to nie trzba używać)\n",
        "IDX = 1200\n",
        "X_train = X_train[:IDX]\n",
        "y_train = y_train[:IDX]\n",
        "assert X_train.shape[0] == y_train.shape[0]\n",
        "assert X_test.shape[0] == y_test.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0746cb3",
      "metadata": {
        "id": "d0746cb3"
      },
      "outputs": [],
      "source": [
        "tk = Tokenizer(num_words=NB_WORDS,\n",
        "               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "               lower=True,\n",
        "               split=\" \")\n",
        "# PROSZĘ UZUPEŁNIĆ:wyuczenie powyższego tokenizera na tekście treningowym\n",
        "\n",
        "\n",
        "#PROSZĘ UZUPEŁNIĆ:Inferencja wyuczonego tokenizera na tekście treningowym i testowym\n",
        "X_train_seq = None\n",
        "X_test_seq = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "665ad567",
      "metadata": {
        "id": "665ad567"
      },
      "outputs": [],
      "source": [
        "seq_lengths = X_train.apply(lambda x: len(x.split(' ')))\n",
        "seq_lengths.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a18bdf73",
      "metadata": {
        "id": "a18bdf73"
      },
      "outputs": [],
      "source": [
        "#PROSZĘ UZUPEŁNIĆ: Padding sekwencji do wartości MAX_LEN (treningowej i testowej)\n",
        "X_train_seq_trunc = None\n",
        "X_test_seq_trunc = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ea15bce",
      "metadata": {
        "id": "1ea15bce"
      },
      "outputs": [],
      "source": [
        "# X_test_seq_trunc[10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1703093",
      "metadata": {
        "id": "f1703093"
      },
      "outputs": [],
      "source": [
        "#Zastosowanie label encodera na sentymencie i stworzenie wektorów y\n",
        "le = LabelEncoder()\n",
        "#PROSZĘ UZUPEŁNIĆ: wytrenowanie na treningowym y Label Encodera oraz infernecja na teście\n",
        "y_train_le = None\n",
        "y_test_le = None\n",
        "#PROSZĘ UZUPEŁNIĆ: stworzenie y kategorycznych - treningowych i testowych\n",
        "y_train_oh = None\n",
        "y_test_oh = None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "j4_54GsibwXd"
      },
      "id": "j4_54GsibwXd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e62fb08e",
      "metadata": {
        "id": "e62fb08e"
      },
      "source": [
        "## One Hot embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45782908",
      "metadata": {
        "id": "45782908"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "result_train = []\n",
        "for seq in X_train_seq_trunc:\n",
        "    result_train.append(to_categorical(seq, num_classes=NB_WORDS))\n",
        "    \n",
        "result_test = []\n",
        "for seq in X_test_seq_trunc:\n",
        "    result_test.append(to_categorical(seq, num_classes=NB_WORDS))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06e30006",
      "metadata": {
        "id": "06e30006"
      },
      "outputs": [],
      "source": [
        "X_train_oh = np.array(result_train)\n",
        "X_test_oh = np.array(result_test)\n",
        "X_train_oh.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5a69695",
      "metadata": {
        "id": "a5a69695"
      },
      "outputs": [],
      "source": [
        "oh_model = models.Sequential()\n",
        "#PROSZĘ UZUPEŁNIĆ: dodanie następujących warst do modelu:\n",
        "# - warstwa wejsciowa o odpowiednim kształcie!\n",
        "# - warstwa spłaszczająca\n",
        "# - warstwa gęsta z odpowiednią liczbą neuronów (sprawdź y) i funkcją softmax\n",
        "\n",
        "oh_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e7adb3b",
      "metadata": {
        "id": "0e7adb3b"
      },
      "outputs": [],
      "source": [
        "oh_history = deep_model(oh_model, X_train_oh, y_train_oh, X_test_oh, y_test_oh)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e7633b1",
      "metadata": {
        "id": "4e7633b1"
      },
      "outputs": [],
      "source": [
        "oh_history.history['accuracy'][-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6810adb8",
      "metadata": {
        "id": "6810adb8"
      },
      "outputs": [],
      "source": [
        "oh_results = test_model(oh_model, X_train_oh, y_train_oh, X_test_oh, y_test_oh, 3)\n",
        "print('/n')\n",
        "print('Test accuracy of word glove model: {0:.2f}%'.format(oh_results[1]*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c266add4",
      "metadata": {
        "id": "c266add4"
      },
      "outputs": [],
      "source": [
        "eval_metric(oh_history, 'loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "783d4a8f",
      "metadata": {
        "id": "783d4a8f"
      },
      "outputs": [],
      "source": [
        "eval_metric(oh_history, 'accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57abc2ed",
      "metadata": {
        "id": "57abc2ed"
      },
      "source": [
        "## Glove embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3fdee29",
      "metadata": {
        "id": "d3fdee29"
      },
      "outputs": [],
      "source": [
        "# #Glove 6B\n",
        "\n",
        "!curl -OL http://nlp.stanford.edu/data/glove.6B.zip -o glove.6B.zip\n",
        "# #wget\n",
        "# # !wget http://nlp.stanford.edu/data/glove.6B.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip -o glove.6B.zip\n",
        "!unzip -o /content/glove.6B.zip"
      ],
      "metadata": {
        "id": "3RHsUxP9csbE"
      },
      "id": "3RHsUxP9csbE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce00b300",
      "metadata": {
        "id": "ce00b300"
      },
      "outputs": [],
      "source": [
        "glove_embeddings = {}\n",
        "with open('glove.6B.300d.txt') as f:\n",
        "    glove_embeddings = {l.split()[0]: np.array(l.split()[1:]).astype('float32') for l in f}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c3d8380",
      "metadata": {
        "id": "6c3d8380"
      },
      "outputs": [],
      "source": [
        "airline_words = ['airplane', 'airline', 'flight', 'luggage', 'djfhaskdjfasdf']\n",
        "for w in airline_words:\n",
        "    if w in glove_embeddings.keys():\n",
        "        print('Found the word {} in the dictionary'.format(w))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_closest(glove_embeddings['airplane'], glove_embeddings)"
      ],
      "metadata": {
        "id": "DylO8UJpeW6q"
      },
      "id": "DylO8UJpeW6q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54c953a7",
      "metadata": {
        "id": "54c953a7"
      },
      "outputs": [],
      "source": [
        "#stworzenie macierzy mapowania pomiędzy słowami a odpowiadającymi im wektorami\n",
        "emb_matrix = np.zeros((NB_WORDS, GLOVE_DIM))\n",
        "\n",
        "for w, i in tk.word_index.items():\n",
        "    # The word_index contains a token for all words of the training data so we need to limit that\n",
        "    if i < NB_WORDS:\n",
        "        vect = glove_embeddings.get(w)\n",
        "        # Check if the word from the training data occurs in the GloVe word embeddings\n",
        "        # Otherwise the vector is kept with only zeros\n",
        "        if vect is not None:\n",
        "            emb_matrix[i] = vect\n",
        "    else:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1988bbee",
      "metadata": {
        "id": "1988bbee"
      },
      "outputs": [],
      "source": [
        "glove_model = models.Sequential()\n",
        "#PROSZĘ UZUPEŁNIĆ: dodanie następujących warst do modelu:\n",
        "# - warstwa embedding o odpowiednim kształcie! (wykorzystaj NB_WORDS, GLOVE_DIM i MAX_LEN)\n",
        "# - warstwa spłaszczająca\n",
        "# - warstwa gęsta z odpowiednią liczbą neuronów (sprawdź y) i funkcją softmax\n",
        "\n",
        "glove_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69b57e57",
      "metadata": {
        "id": "69b57e57"
      },
      "outputs": [],
      "source": [
        "#UWAGA - musimy zablokować możliwość trenowania warstwy z embeddingami glove!\n",
        "glove_model.layers[0].set_weights([emb_matrix])\n",
        "glove_model.layers[0].trainable = False\n",
        "glove_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32a956d8",
      "metadata": {
        "id": "32a956d8"
      },
      "outputs": [],
      "source": [
        "glove_history = deep_model(glove_model, X_train_seq_trunc, y_train_oh, X_test_seq_trunc, y_test_oh)\n",
        "glove_history.history['accuracy'][-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff287faa",
      "metadata": {
        "id": "ff287faa"
      },
      "outputs": [],
      "source": [
        "glove_results = test_model(glove_model, X_train_seq_trunc, y_train_oh, X_test_seq_trunc, y_test_oh, 3)\n",
        "print('/n')\n",
        "print('Test accuracy of word glove model: {0:.2f}%'.format(glove_results[1]*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8c8161c",
      "metadata": {
        "id": "c8c8161c"
      },
      "outputs": [],
      "source": [
        "eval_metric(glove_history, 'loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bc72277",
      "metadata": {
        "id": "4bc72277"
      },
      "outputs": [],
      "source": [
        "eval_metric(glove_history, 'accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02f6851f",
      "metadata": {
        "id": "02f6851f"
      },
      "source": [
        "## Extra section: Trainable Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c905b55e",
      "metadata": {
        "id": "c905b55e"
      },
      "outputs": [],
      "source": [
        "emb_model = models.Sequential()\n",
        "#PROSZĘ UZUPEŁNIĆ: dodanie następujących warst do modelu:\n",
        "# - warstwa embedding o odpowiednim kształcie - użyj wymiarowości wektora 8 (wykorzystaj NB_WORDS i MAX_LEN)\n",
        "# - warstwa spłaszczająca\n",
        "# - warstwa gęsta z odpowiednią liczbą neuronów (sprawdź y) i funkcją softmax\n",
        "\n",
        "emb_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b19b15a9",
      "metadata": {
        "id": "b19b15a9"
      },
      "outputs": [],
      "source": [
        "emb_history = deep_model(emb_model, X_train_seq_trunc, y_train_oh, X_test_seq_trunc, y_test_oh)\n",
        "emb_history.history['accuracy'][-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2377bc76",
      "metadata": {
        "id": "2377bc76"
      },
      "outputs": [],
      "source": [
        "emb_results = test_model(emb_model, X_train_seq_trunc, y_train_oh, X_test_seq_trunc, y_test_oh, 3)\n",
        "print('/n')\n",
        "print('Test accuracy of word glove model: {0:.2f}%'.format(emb_results[1]*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b48e94da",
      "metadata": {
        "id": "b48e94da"
      },
      "outputs": [],
      "source": [
        "eval_metric(emb_history, 'loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32a819df",
      "metadata": {
        "id": "32a819df"
      },
      "outputs": [],
      "source": [
        "eval_metric(emb_history, 'accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f42682e1",
      "metadata": {
        "id": "f42682e1"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "nlp"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "name": "Copy of Embeddings_vs_One_Hot-plain.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}